{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Luke_Hayes_14498098_Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oP1uun77cIh"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "This assignment will involve the creation of a spellchecking system and an evaluation of its performance. You may use the code snippets provided in Python for completing this or you may use the programming language or environment of your choice\n",
        "\n",
        "Please start by downloading the corpus `holbrook.txt` from Blackboard\n",
        "\n",
        "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
        "\n",
        "    My siter|sister go|goes to Tonbury .\n",
        "    \n",
        "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
        "\n",
        "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
        "\n",
        "    My Mum goes out some_times|sometimes .\n",
        "    \n",
        "For the purpose of this assignment you do not need to separate these words, but instead you may treat them like a single token.\n",
        "\n",
        "*Note: you may use any functions from NLTK to complete the assignment. It should not be necessary to use other libraries and so please consult with us if your solution involves any other external library. If you use any function from NLTK in Task 6 please include a brief description of this function and how it contributes to your solution.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIVCSJV-7kDs"
      },
      "source": [
        "## Task 1 (10 Marks)\n",
        "\n",
        "Write a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes. The indexes refers to the index of the words in the sentence. In the example given, there is only an error in the 10th word and so the list of indexes is [9]. It is not necessary to analyze where the error occurs inside the word.\n",
        "\n",
        "Then split your data into a test set of 100 lines and a training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RznCZ1mw7mfk"
      },
      "source": [
        "import nltk\n",
        "#nltk.download()\n",
        "#read in the text\n",
        "lines = open(\"holbrook.txt\").readlines()\n",
        "data = []\n",
        "total_lines = len(lines)\n",
        "corrected_list = []\n",
        "original_list = []\n",
        "\n",
        "\n",
        "for i in range(total_lines):\n",
        "\n",
        "  #split up each line of text into works\n",
        "  lines_split = lines[i].split(\" \")\n",
        "  #lists for storing respective sentences and indexes - will be set to null at the start of each new sentence\n",
        "  original_sentence = []\n",
        "  corrected_sentence = []\n",
        "  indexes = []\n",
        "  for j in range(len(lines_split)):\n",
        "\n",
        "    #here we remove any of the \\n at the end of sentences\n",
        "    #note we do not tokenize until later as it feels easier to work with the strings here \n",
        "    if \"\\n\" in lines_split[j]:\n",
        "      #use strip to remove the \\n\n",
        "      lines_split[j] = lines_split[j].rstrip(\"\\n\")\n",
        "\n",
        "    #here we need to split the two words either side of the | symbol\n",
        "    if \"|\" in lines_split[j]:\n",
        "      #we add the index at which this symbol exists to the list of indexes\n",
        "      indexes.append(j)\n",
        "      #we use split to split the word with the symbol into two words with no symbol\n",
        "      word_split = lines_split[j].split(\"|\")\n",
        "      #the first word is the mispelled word so we add that to the original word list\n",
        "      original_sentence.append(word_split[0])\n",
        "      #the second word is the corrected version of the word so we add that to its corresponding list\n",
        "      corrected_sentence.append(word_split[1])\n",
        "\n",
        "      #here we could do a quick check to see if the word is unique\n",
        "\n",
        "    #if there is no symbol we just add the word to both lists\n",
        "    else:\n",
        "      original_sentence.append(lines_split[j])\n",
        "      corrected_sentence.append(lines_split[j])\n",
        "  \n",
        "  #here we create a dictionary item for each sentence and the indexes\n",
        "  #the values i.e. the sentences and indexes are in array form and we can acces these using their corresponding keys\n",
        "  info = {\n",
        "        \"original\": original_sentence,\n",
        "        \"corrected\": corrected_sentence,\n",
        "        \"indexes\":  indexes,\n",
        "  }\n",
        "\n",
        "  #add each of the sentences to their respective lists\n",
        "  corrected_list.append(corrected_sentence)\n",
        "  original_list.append(original_sentence)\n",
        "  \n",
        "  #add each dictionary item to a data list\n",
        "  data.append(info)\n",
        "\n",
        "#assert(data[2] == {\n",
        "#    'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], \n",
        "#    'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], \n",
        "#    'indexes': [9]\n",
        "#})"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRSX4I0H7pSC"
      },
      "source": [
        "The counts and assertions given in the following sections are based on splitting the training and test set as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt9aR2Gy7p1C"
      },
      "source": [
        "#split up our data into lists that will be needed later\n",
        "test = data[:100]\n",
        "train = data[100:]\n",
        "test_corrected = corrected_list[:100]\n",
        "test_original = original_list[:100]\n",
        "train_corrected = corrected_list[100:]\n",
        "train_original = original_list[100:]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm5JL7cH7sLK"
      },
      "source": [
        "## **Task 2** (10 Marks): \n",
        "Calculate the frequency (number of occurrences), *ignoring case*, of all words and their unigram probability from the corrected *training* sentences.\n",
        "\n",
        "*Hint: use `Counter` to implement this so it may be called many times*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ge0uHS-7uEK",
        "outputId": "3197adb4-4aca-4406-a336-7bd9143ea7e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from collections import Counter\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import itertools\n",
        "\n",
        "#use itertools to iterate over the list of lists(sentences) and put them in the same list\n",
        "train_corrected_list = list(itertools.chain.from_iterable(train_corrected))\n",
        "#this list of unique words will be used throughout the assignment\n",
        "unique_words = []\n",
        "\n",
        "for i in train_corrected_list:\n",
        "  if i.lower() not in unique_words:\n",
        "    unique_words.append(i.lower())\n",
        "\n",
        "#function to get the frequency of the word\n",
        "def unigram(word):\n",
        "  #use counter to return a counter dictionary of all the words and their frequencies as key-value pairs\n",
        "  word_counter = Counter(train_corrected_list)\n",
        "  #lookup the frequency of the word we need\n",
        "  frequency = word_counter[word]\n",
        "  #return that frequency\n",
        "  return frequency \n",
        "    \n",
        "#function to get the probability\n",
        "def prob(word):\n",
        "  #call the unigram function to get the frequency value of the word\n",
        "  frequency = unigram(word)\n",
        "  #get the total number of all occurences of all words and sum them together\n",
        "  total_word_num = sum(Counter([word for word in train_corrected_list]).values())\n",
        "  #divide the frequency of the word by the total sum of all words in the train corrected list\n",
        "  probability = frequency/total_word_num\n",
        "  #return this probability\n",
        "  return probability\n",
        "\n",
        "prob(\"me\")\n",
        "\n",
        "# Test your code with the following\n",
        "#assert(unigram(\"me\")==87)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.004031697483664674"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8r8QYj78GPK"
      },
      "source": [
        "## **Task 3** (15 Marks): \n",
        "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV9Mu8P38IQE",
        "outputId": "8e0e13b1-8b84-468a-ff4f-90ccdeb82569",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "# Edit distance returns the number of changes to transform one word to another\n",
        "print(edit_distance(\"hello\", \"hi\"))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm46Lbiz8K8M"
      },
      "source": [
        "Write a function that calculates all words with *minimal* edit distance to the misspelled word. You should do this as follows\n",
        "\n",
        "1. Collect the set of all unique tokens in `train`\n",
        "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
        "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n",
        "\n",
        "*Do not implement edit distance, use the built-in NLTK function `edit_distance`*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoilAmFW8PCb",
        "outputId": "58571321-52f8-40bd-c4aa-092df66ffc5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def get_candidates(token):\n",
        "\n",
        "  #create a dictionary \n",
        "  distance_list = {}\n",
        "  #create list of candidates\n",
        "  candidates = []\n",
        "  #for each word in the unique words list and the word given we are going to call edit_distance \n",
        "  #this will basically return the number of changes it would take two words the same\n",
        "  #e.g. mine and mind = 1 change\n",
        "  #for this word we store all of its distances for all the other unique words\n",
        "  for word in unique_words:\n",
        "    #get the distance between the word and the unique word\n",
        "    val = edit_distance(token, word)\n",
        "    #create a dictionary item for each distance\n",
        "    dist = {word: val}\n",
        "    #add the distance to a list so now we will have all the distances for the word with all the unique words\n",
        "    distance_list.update(dist)\n",
        "\n",
        "  #we need to sort this list so we can get the closest distance or a list of the closest distances if a few have the same value\n",
        "  #we sort by value as this is the number\n",
        "  sorted_list = sorted(((value, key) for (key,value) in distance_list.items()))\n",
        "  \n",
        "  #now we take the key and value with the lowest distance\n",
        "  #we use the min function\n",
        "  minimum = min(sorted_list)\n",
        "\n",
        "  #there can be many candidates with the same distance value so we must return them also\n",
        "  for value,key in sorted_list:\n",
        "    #if the value is equal to the value of the minimum value we add it to the clist of canddiates and return it\n",
        "    if value == minimum[0]:\n",
        "      candidates.append(key)\n",
        "\n",
        "  return candidates\n",
        "  \n",
        "get_candidates(\"minde\")  \n",
        "# Test your code as follows\n",
        "#assert get_candidates(\"minde\") == ['mine', 'mind']"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mind', 'mine']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGY-eCkN8TIM"
      },
      "source": [
        "## Task 4 (15 Marks):\n",
        "\n",
        "Write a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary (set of unique words in the training set) and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *unigram probability*. \n",
        "\n",
        "*Your solution to this should involve `get_candidates`*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIGKE4_P8WGP",
        "outputId": "412470da-445b-4f1a-d8d1-0f48db299556",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def correct(sentence):\n",
        "    #create list to append the corrected sentence to\n",
        "    corrected_sentence = []\n",
        "\n",
        "    #loop through the words in the sentence\n",
        "    for word in sentence:\n",
        "\n",
        "      #if the word is not in the list of unique words i.e. the training list we must try a prediction\n",
        "      #all out unique words are in lower case\n",
        "      if word.lower() not in unique_words:\n",
        "        \n",
        "        #call get_candidated to return a list of the words with the least amount of changes to the word\n",
        "        candidates = get_candidates(word.lower())\n",
        "        #create a list to store the probailities\n",
        "        prob_list = []\n",
        "\n",
        "        #if there is only one candidate we can append it to the list holding the corrected sentence\n",
        "        #as it is the only ligitimite answer\n",
        "        if len(candidates) == 1:\n",
        "          corrected_sentence.append(candidates[0])\n",
        "\n",
        "        #otherwise we need to find the probabilities of each of the candidates\n",
        "        elif len(candidates) >= 1:\n",
        "          #for each candidate get its probability and add it to a list of probabilities for all candidates\n",
        "          for candidate in candidates:\n",
        "            prob_list.append(prob(candidate))\n",
        "\n",
        "          #get the index of the max probability which will mirror the list of candidates\n",
        "          maxidx = prob_list.index(max(prob_list))\n",
        "          #then we use that index to add the candidate who has the largest probability\n",
        "          corrected_sentence.append(candidates[maxidx])\n",
        "      \n",
        "      #if the word is in the unique words we do not need to predict\n",
        "      else: \n",
        "        corrected_sentence.append(word)\n",
        "    return corrected_sentence\n",
        "\n",
        "correct([\"this\",\"whitr\",\"cat\"])\n",
        "#assert(correct([\"this\",\"whitr\",\"cat\"]) = ['this','white','cat'])   "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'white', 'cat']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG7jC6au8kka"
      },
      "source": [
        "## **Task 5** (10 Marks): \n",
        "Using the test corpus evaluate the *accuracy* of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSXTQypR8mdR",
        "outputId": "ba5458b0-12ce-493a-afc3-1b6beca6dd1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def accuracy(test):\n",
        "  fixed_sentence = []\n",
        "  incorrect_sentence_counter = 0\n",
        "  correct_sentence_counter = 0\n",
        "  count_incorrect_words = 0\n",
        "  count_correct_words = 0\n",
        "\n",
        "  #call the correct sentence method on each sentence in the list passed in\n",
        "  [fixed_sentence.append(correct(sentence)) for sentence in test]\n",
        "\n",
        "  #loop through the length of the list to compare results\n",
        "  for i in range(len(test_corrected)):\n",
        "\n",
        "    #if the sentences are the same incremement the counter\n",
        "    if (fixed_sentence[i] == test_corrected[i]):\n",
        "      correct_sentence_counter = correct_sentence_counter + 1\n",
        "    else:\n",
        "      #print(fixed_sentence[i])\n",
        "      #print(test_corrected[i])\n",
        "      #print(test_original[i])\n",
        "      incorrect_sentence_counter = incorrect_sentence_counter + 1\n",
        "\n",
        "  #put all the lists into one list with all their relevent words in that list so we can zip and compare\n",
        "  fixed_list = list(itertools.chain.from_iterable(fixed_sentence))\n",
        "  corrected_list = list(itertools.chain.from_iterable(test_corrected))\n",
        "  original_list = list(itertools.chain.from_iterable(test_original))\n",
        "\n",
        "\n",
        "  #zip all the files and loop through so we can compare the words of predicted and correct ang get results\n",
        "  for x, y, z in zip(fixed_list, corrected_list,original_list):\n",
        "      if x == y:\n",
        "        count_correct_words = count_correct_words + 1\n",
        "      else:\n",
        "        #print(\"Predicted: %s, Expected: %s, Start word: %s \" %(x, y, z))\n",
        "        count_incorrect_words = count_incorrect_words + 1\n",
        "\n",
        "  print(\"This is the number of total incorrect words: %d \" %count_incorrect_words)\n",
        "  print(\"This is the number of total correct words: %d \" %(count_correct_words))\n",
        "  count_correct_words = float(count_correct_words)\n",
        "  compare_list = float(len(corrected_list))\n",
        "  print(\"This is the percentage of correctly predicted words: %f\" %((count_correct_words/compare_list) * 100) ,\"%\")\n",
        "  print(\"This is the percentage of correctly predicted sentences: %f\" %((correct_sentence_counter/(incorrect_sentence_counter+correct_sentence_counter)) * 100) ,\"%\")\n",
        "  #we have implemented it on sentences now we need to do so for words\n",
        "  #so we therefore need to create a list with all the words from each of these \n",
        "\n",
        "\n",
        "\n",
        "accuracy(test_original)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is the number of total incorrect words: 181 \n",
            "This is the number of total correct words: 948 \n",
            "This is the percentage of correctly predicted words: 83.968113 %\n",
            "This is the percentage of correctly predicted sentences: 21.000000 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b-r2JzD8_Zh"
      },
      "source": [
        "## **Task 6 (35 Marks):**\n",
        "\n",
        "Consider a modification to your algorithm that would improve the accuracy of the algorithm developed in Task 3 and 4\n",
        "\n",
        "* You may resources beyond those provided here.\n",
        "* You must **not use the test data** in this task.\n",
        "* Provide a short text describing what you intend to do and why. \n",
        "* Full marks for this section may be obtained without an implementation, but an implementation is preferred.\n",
        "* Your implementation should not consist of more than 50 lines of code\n",
        "\n",
        "Please note this task is marked according to: demonstration of knowledge from the lectures (10), originality and appropriateness of solution (10), completeness of description (10) and technical correctness (5).\n",
        "\n",
        "My approach is to aim to try and make corrections for as few words as possible. The reason for doing this is that we only want change words that afre mispelled. My algorithm gets **91.674048%** of the words correct. There are a total of 94 words wrongly predicted (181 in task 5) and the majority of these are quite tough cases. For example we are given the word go and we are meant to predict goes. Perhaps better tense detection would help fix this but it is a difficult case. I believe that after analysing the 94 incorrectly predicted words the percentage accuracy I have achieved is very good.  \n",
        "\n",
        "\n",
        "**Named Entities**\n",
        "\n",
        "Some examples of Named Entities are peoples names, places, organizations and times. This process uses the Part of Speech tags and tries to identify nouns. Named Entity Detection utilizes a technique called chunking which goups words together to see if multiple words make one a single named entity e.g. New York. These chunks are called NP chunks or Noun Phrase chunks. Nour Phrase chunks are made up of a determiner (DT) followed bu an adjective (ADJ) and then followed by a noun. In my method we simply pass the word to the named entity checker so we do not ulitize the full capabilities of this prediction. However, there are no named entities in this example where it is made up of two or more words so this limitation does not hunder the accuracy. ne_chunk returns a tree object so I check to see if any node on the tree has a named entity label and if it does I add the word to a list of named entities and don't try to make an correction for that word.\n",
        "\n",
        "\n",
        "**Bigrams**\n",
        "\n",
        "In the first approach to this question we used the prob method which simply selected the word with the highest probability from the possible candidates from all of the words that required the least amount of changes from the mispelt word. A better approach here was to use bigrams which now takes into account the previous word before the word we are trying to predict. Therefore we get the probability of the previous word along with each of the words that are our possible candidates and then we select the highest. \n",
        "\n",
        "\n",
        "We also use **Tense Detection** in conjuction with this. The reason for doing so is that when we are looking to choose the highest bigram probability quite often we are given a set of words all with the same bigram probability. The idea of using tense detection here is to make sure that the presented words are of the same tense of the mispelled word. We must note that the fact that the word is mispelled will cause the tense of that word to possibly change. I did also try to use tense detection to fix words after they had been corrected but I found that my final approach worked better. Tense Detection works by using the Part of Speech tag to see if the word is past, future or present in tense. This is difficult because I am looking to classify mispelt words only. \n",
        "\n",
        "**Add-one Smoothing**\n",
        "\n",
        "When calculating the probability for each bigram there are a huge amount of bigram pairs that have never been seen before together and therefore the probabilitiy of this bigram occuring is 0. However, this is inaccuarate as we know these words can occur together. To deal with this we implement a process called add-one smoothing. To do this we set the number of times the bigram pair occur together to 1. It is important then that the probability of the bigrams where add-one smoothing has been implements are less likely to occur than the bigram pairs that actually did appear once. Therefore when calculating the probability of a bigram pair after implementing add-one smoothing is 1/(frequency of word 1 + 1) while all other words that actuallly appear once will have a probability of 1/frequency of word 1. It is also important to note that for this to be more accurate we should have not counted the frequencies of word one where it was the last word of a sentence as it has no bigram in that case.\n",
        "\n",
        "**Trigrams**\n",
        "\n",
        "Trigrams could also have been implemented. With trigrams we group three words together so when we are trying to correct a word we look at the two words that occur before it and all the candidates. We will then select the highest trigram probability. We would simply have to alter this line of code in the bigram_probability method:   \n",
        "\n",
        "```\n",
        "train_bigrams = nltk.ngrams(corrected_list, 2):\n",
        "#to\n",
        "train_bigrams = nltk.ngrams(corrected_list, 3)\n",
        "```\n",
        "This may or may not have given a better outcome. \n",
        "\n",
        "\n",
        "**Extra Items Implemented to Increase Accuracy**\n",
        "1. We do not make any changes if a **token is a digit or if it contains any digits**. The reason for this is that without this we would be carrying out predictions for number and were therefore changign numbers to the numbers that were in the list of unique words. So if the token contains a digit we do not do any prediction.\n",
        "2. If the word we are trying to predict is a **real english word** we do not do any word prediction as I assume that the word is correct. After analyzing the words I was incorrectly predicting I found that I was making predictions for words that were already correct and did not need to be altered. To fix this issue I created a method is_word_real which checks if the word is located in the WordNet corpus of english words. If the word is in the database we return true and then don't do any prediction for that word.\n",
        "3. Finally if the **word's first letter is a capital letter** we do not do any prediction. I implemented this as there were many some placenames and accronyms such as 'Bullimore' which were not picked up as named entities and therefore predictions were being made on these words that were in fact already correct. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dxHOcchYJG-",
        "outputId": "6e6d8896-c25b-4119-fb2d-37484912aa50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#let's put all our new functions in here \n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk import pos_tag, ne_chunk, word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import words\n",
        "\n",
        "#create list to hold all named entities\n",
        "named_entity_list = []\n",
        "\n",
        "#checks if the word we are going to predict is a named entity\n",
        "def named_entity_check(word):\n",
        "\n",
        "  #created a tree using chunking\n",
        "  #ideally we should maybe do this on the full sentence for better results but it appears to work well\n",
        "  #does really well at removing errors of places and names of people and organizations\n",
        "  tree = ne_chunk(nltk.pos_tag(nltk.word_tokenize(word)))\n",
        "  #only add if its not already in the list \n",
        "  for t in tree:\n",
        "    if hasattr(t, 'label'):\n",
        "        if word not in named_entity_list:\n",
        "          named_entity_list.append(word)\n",
        "\n",
        "#named_entity_check(\"Google\")\n",
        "\n",
        "\n",
        "\n",
        "#we use this function to return the tense of a supplied word\n",
        "#when we try to predict words using bigrams in most cases the bigrams of the possible words have never occured before\n",
        "#this means we often have lots of words with the same probability with no stand out word\n",
        "#to help with this we try to remove word that are not relevent by getting the tense of all candidates and only keeping the candidates \n",
        "#with the same tense as the word we are trying to predict\n",
        "#this can be difficult as often time the word is mispelled but it is an attempt to improve accuracy\n",
        "#sometimes an irrelevent word will have a higher probability than the actual correct word so this will help remove those cases\n",
        "def tense_detection(word):\n",
        "  #get the part of speech tag\n",
        "  word_tag = dict(pos_tag(word_tokenize(word)))\n",
        "  #put it in a list and return it\n",
        "  tag = list(word_tag.values())\n",
        "\n",
        "  return tag\n",
        "  \n",
        "def bigram_probability(word1, word2):\n",
        "\n",
        "  #we use the corrected train list as our bigram training list\n",
        "  #we do the following to put all the strings into one list rather than a list of sentences with each sentence being a list \n",
        "  corrected_list = list(itertools.chain.from_iterable(train_corrected))\n",
        "\n",
        "  #we then use the n-grams function to create bigrams from the \n",
        "  #if we were to put 3 in here we could have created tri-grams\n",
        "  train_bigrams = nltk.ngrams(corrected_list, 2)\n",
        "\n",
        "  #we use counter to create \n",
        "  bigrams_frequency = Counter(train_bigrams)\n",
        "  #print(bigrams_frequency)\n",
        "  frequency_of_pair = bigrams_frequency[(word1, word2)]\n",
        "\n",
        "  #here we get the number of times each word occurs\n",
        "  #ideally here we should have some functionality that doesn't count the word if it is at the end of a sentence(as it has no bigram in this case)\n",
        "  #therefore this calculation is not 100 percent accurate\n",
        "  bigram_counter = Counter(corrected_list)\n",
        "\n",
        "  #now we use the counter above to get the total number of occurences of word 1\n",
        "  word_count = bigram_counter[word1]\n",
        "\n",
        "  #add exception because word1 may never occur\n",
        "  try:\n",
        "    #out probability is the number of times the words occur as a pair divided by the number of times word 1 appears\n",
        "    #again this should have something to remove the occurences where word 1 is at the start of a sentence\n",
        "    probability = frequency_of_pair / word_count\n",
        "  #error is handled and then we will use add-one smoothening below\n",
        "  except ZeroDivisionError:\n",
        "    probability = 0\n",
        "\n",
        "\n",
        "  # Add-one smoothing for occurences where the pair does not occur in the training set\n",
        "  if probability == 0:\n",
        "\n",
        "    #In this case add one to the frequency of the pair\n",
        "    frequency_of_pair = frequency_of_pair + 1\n",
        "    #add one to the total number of times word 1 appears\n",
        "    word_count = bigram_counter[word1] + 1\n",
        "    #divide the two to get the bigram porbability\n",
        "    probability = frequency_of_pair / word_count\n",
        "\n",
        "  return probability\n",
        "\n",
        "#This function is called to see if a word is a real word\n",
        "#I try to limit word prediction to words that are spelled wrongly\n",
        "#this is to limit the incorrect prediction of words that are already correct\n",
        "#if the word is not present in wordnet synsets then we assume it is not a proper word\n",
        "def is_word_real(word):\n",
        "    if not wordnet.synsets(word):\n",
        "      return False\n",
        "    else:\n",
        "      return True\n",
        "\n",
        "\n",
        "bigram_probability(\"I\", \"wash\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0019880715705765406"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWgQsZBbYR4F",
        "outputId": "ec668df7-edcd-461c-91f6-50ebc6a00fdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def correct_new(sentence):\n",
        "\n",
        "    #list to hold the sentence after we correct it\n",
        "    corrected_sentence = []\n",
        "    #this is going to store word-1 which is the word before the word we are trying to predict\n",
        "    #this will of course be needed for the bigram probability calculation\n",
        "    bigram_first_word = sentence[0]\n",
        "    #this variable will say if the word is the first word of the sentence\n",
        "    #if it is the first word we don't use bigrams and simply use the old probability calculation for that word only\n",
        "    first_word = True\n",
        "\n",
        "    #run for each word of the sentence\n",
        "    for word in sentence:\n",
        "      #check if the word is a name entity- it will be added to a list if it is\n",
        "      named_entity_check(word)\n",
        "      #this was an attempt to remove the _ in some word but didn't help improve accuracy\n",
        "      if \"_\" in word:\n",
        "        word = word.replace(\"_\", \"\")\n",
        "\n",
        "      #check if the word is a real word in the wordet corpus\n",
        "      english_word = is_word_real(word)\n",
        "\n",
        "      #works as follows we enter if\n",
        "      #1) word is not in our list of unique words\n",
        "      #2) word is not in out list of named entities\n",
        "      #3) if the first letter of the word is not a capital letter\n",
        "      #   - this worked great to remove errors of words as if they were capitilised they were not likely to be spelled wrong\n",
        "      #4) if the word does not contain digits - removed trying to predict numbers we had not seen\n",
        "      #5) if the word is not an english word i.e. it has been mispelled\n",
        "      if word.lower() not in unique_words and word not in named_entity_list and (word[0].isupper() != True) and not word[0].isdigit() and english_word == False:\n",
        "        lower = word.lower()\n",
        "        #get the list of canddiated\n",
        "        candidates = get_candidates(lower)\n",
        "        prob_list = []\n",
        "\n",
        "        #if there is only 1 candidate append candidate to the corrected sentence\n",
        "        #there is not need to do any work with probabilities\n",
        "        if len(candidates) == 1:\n",
        "          corrected_sentence.append(candidates[0])\n",
        "\n",
        "        #if there are more than 1 candidates we need to get the best candidate which is\n",
        "        #1) matches the tense of the word we were given\n",
        "        #2) get the highest bigram probability of all the candidates after the word that appears before the word we are trying to predict\n",
        "        elif len(candidates) >= 1:\n",
        "\n",
        "          #if the word is the first word in the sentence do normal probability as it has no previous word for bigrams\n",
        "          if first_word == True:\n",
        "            for candidate in candidates:\n",
        "              prob_list.append(prob(candidate))\n",
        "\n",
        "          else: \n",
        "            candidate_tenses = []\n",
        "            for candidate in candidates:\n",
        "              #for each candidate we run tense detection to make sure that the suggestions given to us have the same tense as the mispelled word\n",
        "              candidate_tense = tense_detection(candidate)\n",
        "              candidate_tenses.append(candidate_tense)\n",
        "              incorrect_word_tense = tense_detection(word)\n",
        "\n",
        "            i = 0\n",
        "            k = 0\n",
        "            #copy the list over as we will be removing from the original list\n",
        "            new_candidate_list = list(candidates)\n",
        "\n",
        "            #loop through the list of word candidate tenses\n",
        "            for c in candidate_tenses:\n",
        "              #if the tense mathces the tense of the word we are trying to correct we keep that word and get its bigram probability\n",
        "              #with the predicted word\n",
        "              if c == incorrect_word_tense:\n",
        "                #if we use the old probability method\n",
        "                #prob_list.append(prob(new_candidate_list[k])) \n",
        "                #get the bigram probability and append it to a list of probabilities\n",
        "                prob_list.append(bigram_probability(bigram_first_word,new_candidate_list[k])) \n",
        "                i = i + 1\n",
        "                k = k + 1\n",
        "              \n",
        "              #if the candidate tense doesn't match\n",
        "              else:\n",
        "                #remove it from the list of candidates\n",
        "                candidates.pop(i)\n",
        "                k = k + 1\n",
        "\n",
        "          #we get the index of the max probability in the probability list\n",
        "          #this only contains the matching candidates and therefore is indexed exactly the same as the dandidates list even after removal\n",
        "          maxidx = prob_list.index(max(prob_list))\n",
        "          #we append the best probability to the corrected sentence\n",
        "          corrected_sentence.append(candidates[maxidx])\n",
        "\n",
        "      #if the word doesn't fit the criteria for the first if statement don't do any correction and add to the list\n",
        "      else: \n",
        "        corrected_sentence.append(word)\n",
        "\n",
        "      #now we are moving on to the second word of the sentence\n",
        "      first_word = False\n",
        "      #move the word-1 holder on one place\n",
        "      bigram_first_word = word\n",
        "\n",
        "    #return the corrected sentence\n",
        "    return corrected_sentence\n",
        "\n",
        "correct_new([\"this\",\"whitr\",\"cat\"])\n",
        "#assert(correct([\"this\",\"whitr\",\"cat\"]) = ['this','white','cat'])   \n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'white', 'cat']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLzaC6D28sK9"
      },
      "source": [
        "## **Task 7 (5 Marks):**\n",
        "\n",
        "Repeat the evaluation (as in Task 5) of your new algorithm and show that it outperforms the algorithm from Task 3 and 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw6PzwWn7iEo",
        "outputId": "9f5110c1-9525-4d78-863e-edbd50e74fd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def accuracy_new(test):\n",
        "\n",
        "  fixed_sentence = []\n",
        "\n",
        "  #call the new correct method on all the sentences we are trying to test\n",
        "  [fixed_sentence.append(correct_new(sentence)) for sentence in test]\n",
        "\n",
        "  correct_sentence_counter = 0\n",
        "  incorrect_sentence_counter = 0\n",
        "  correct_words_counter = 0\n",
        "  incorrect_words_counter = 0\n",
        "  \n",
        "  #here we compare each sentence to see how may are correct\n",
        "  for i in range(len(test_corrected)):\n",
        "    if (fixed_sentence[i] == test_corrected[i]):\n",
        "      correct_sentence_counter = correct_sentence_counter + 1\n",
        "    else:\n",
        "      #print(fixed_sentence[i])\n",
        "      #print(test_corrected[i])\n",
        "      #print(test_original[i])\n",
        "      incorrect_sentence_counter = incorrect_sentence_counter + 1\n",
        "\n",
        "  #make each list of lists into a single list with all their respective words\n",
        "  fixed_list = list(itertools.chain.from_iterable(fixed_sentence))\n",
        "  corrected_list = list(itertools.chain.from_iterable(test_corrected))\n",
        "  original_list = list(itertools.chain.from_iterable(test_original))\n",
        "\n",
        "\n",
        "  #here we will compare the words with each other to see how many are wrong\n",
        "  for x, y, z in zip(fixed_list, corrected_list, original_list):\n",
        "      if x == y:\n",
        "        correct_words_counter = correct_words_counter + 1\n",
        "      else:\n",
        "        #print(\"Predicted: %s, Expected: %s, Start word: %s \" %(x, y, z))\n",
        "        incorrect_words_counter = incorrect_words_counter + 1\n",
        "\n",
        "  print(\"This is the number of total incorrect words: %d \" %incorrect_words_counter)\n",
        "  print(\"This is the number of total correct words: %d \" %(correct_words_counter))\n",
        "  correct_words_counter = float(correct_words_counter)\n",
        "  compare_list = float(len(corrected_list))\n",
        "  print(\"This is the percentage of correctly predicted words: %f\" %((correct_words_counter/compare_list) * 100) ,\"%\")\n",
        "  print(\"This is the percentage of correctly predicted sentences: %f\" %((correct_sentence_counter/(incorrect_sentence_counter+correct_sentence_counter)) * 100) ,\"%\")\n",
        "\n",
        "accuracy_new(test_original)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is the number of total incorrect words: 94 \n",
            "This is the number of total correct words: 1035 \n",
            "This is the percentage of correctly predicted words: 91.674048 %\n",
            "This is the percentage of correctly predicted sentences: 40.000000 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}